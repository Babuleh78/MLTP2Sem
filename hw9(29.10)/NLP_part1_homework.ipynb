{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xRUXhCVUzur"
   },
   "source": [
    "# Домашнее задание \"NLP. Часть 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "koQiHQFT8XO7"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "#import fasttext\n",
    "#import fasttext.util Эти библиотеки несовместимы с моей версией python (3.13), заменил их на другие (см. Задание 5)\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "ZUhaEvmpTCsv"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "_q88wy8uTDZh"
   },
   "outputs": [],
   "source": [
    "def normalize_pretokenize_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "uGDzAEpJT_zs"
   },
   "outputs": [],
   "source": [
    "# This block is for tests only\n",
    "test_corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"never jump over the lazy dog quickly\",\n",
    "    \"brown foxes are quick and dogs are lazy\"\n",
    "]\n",
    "\n",
    "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = normalize_pretokenize_text(text)\n",
    "        all_words.extend(words)\n",
    "    vocab = sorted(set(all_words))\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return vocab, vocab_index\n",
    "\n",
    "vocab, vocab_index = build_vocab(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eemkFZ1tVLw4"
   },
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "Реализовать One-Hot векторизацию текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Qiw7w5OhTDeD"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "def one_hot_vectorization(text: str, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[int]:\n",
    "    vector = [0] * len(vocab) # Вектор длинной в словарь\n",
    "    words = normalize_pretokenize_text(text)\n",
    "     \n",
    "    for word in words: # Устанавливаем 1 для каждого слова\n",
    "        if word in vocab_index:\n",
    "            position = vocab_index[word]\n",
    "            vector[position] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def test_one_hot_vectorization(corpus, vocab, vocab_index) -> bool:\n",
    "    try:\n",
    "        text = \"the quick brown fox\"\n",
    "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
    "\n",
    "        if not isinstance(result, list):\n",
    "            return False\n",
    "\n",
    "        expected_length = len(vocab)\n",
    "        if len(result) != expected_length:\n",
    "            return False\n",
    "\n",
    "        words_in_text = normalize_pretokenize_text(text)\n",
    "        for word in words_in_text:\n",
    "            if word in vocab_index:\n",
    "                idx = vocab_index[word]\n",
    "                if result[idx] != 1:\n",
    "                    return False\n",
    "\n",
    "        print(\"One-Hot-Vectors test PASSED\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "Q2-LJcmbTe04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot-Vectors test PASSED\n"
     ]
    }
   ],
   "source": [
    "assert test_one_hot_vectorization(test_corpus, vocab, vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAF8IOYMVT3s"
   },
   "source": [
    "## Задание 2 (0.5 балла)\n",
    "Реализовать Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "-_QjiviNBkbS"
   },
   "outputs": [],
   "source": [
    "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
    "    words = normalize_pretokenize_text(text)\n",
    "    \n",
    "    bow_dict = {} # Создаем словарь для подсчета частот\n",
    "    \n",
    "    for word in words: # Подсчитываем частоту каждого слова\n",
    "        bow_dict[word] = bow_dict.get(word, 0) + 1\n",
    "    \n",
    "    return bow_dict\n",
    "def test_bag_of_words_vectorization() -> bool:\n",
    "    try:\n",
    "        text = \"the the quick brown brown brown\"\n",
    "        result = bag_of_words_vectorization(text)\n",
    "\n",
    "        if not isinstance(result, dict):\n",
    "            return False\n",
    "\n",
    "        if result.get('the', 0) != 2:\n",
    "            return False\n",
    "        if result.get('quick', 0) != 1:\n",
    "            return False\n",
    "        if result.get('brown', 0) != 3:\n",
    "            return False\n",
    "        if result.get('nonexistent', 0) != 0:\n",
    "            return False\n",
    "\n",
    "        print(\"Bad-of-Words test PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "ScFuXh_9TtJm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad-of-Words test PASSED\n"
     ]
    }
   ],
   "source": [
    "assert test_bag_of_words_vectorization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6LblWJfX2kr"
   },
   "source": [
    "## Задание 3 (0.5 балла)\n",
    "Реализовать TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "RqcMYJkrTlV0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Подсчет idf\n",
    "def compute_idf(corpus, vocab):\n",
    "        idf_dict = {}\n",
    "        total_docs = len(corpus)\n",
    "      \n",
    "        tokenized_corpus = [set(normalize_pretokenize_text(doc)) for doc in corpus]\n",
    "        \n",
    "        for i, word in enumerate(vocab):\n",
    "            doc_count = 0\n",
    "            for doc_tokens in tokenized_corpus:\n",
    "                if word in doc_tokens:\n",
    "                    doc_count += 1\n",
    "           \n",
    "            if doc_count == 0:\n",
    "                idf_dict[word] = math.log(total_docs + 1)  \n",
    "            else:\n",
    "                idf_dict[word] = math.log((total_docs + 1) / (doc_count + 1))\n",
    "        \n",
    "        return idf_dict\n",
    "# Пришлось оптимизировать и кэшировать данные, так как в продакшене этот метод работал очень ОЧЕНЬ медленно\n",
    "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
    "    \n",
    "  \n",
    "    try:\n",
    "        # Быстрая проверка входных данных\n",
    "        if not text or not corpus or not vocab:\n",
    "            return [0.0] * len(vocab) if vocab else []\n",
    "        \n",
    "        # Кэшируем вычисление IDF \n",
    "        if not hasattr(tf_idf_vectorization, 'idf_cache'):\n",
    "            tf_idf_vectorization.idf_cache = {}\n",
    "        \n",
    "        cache_key = f\"{len(corpus)}_{len(vocab)}\"\n",
    "        if cache_key not in tf_idf_vectorization.idf_cache:\n",
    "            tf_idf_vectorization.idf_cache[cache_key] = compute_idf(corpus, vocab)\n",
    "        \n",
    "        idf_dict = tf_idf_vectorization.idf_cache[cache_key]\n",
    "        \n",
    "        # TF-IDF вычисление\n",
    "        vector = [0.0] * len(vocab)\n",
    "        words = normalize_pretokenize_text(text)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return vector\n",
    "        \n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        for word, index in vocab_index.items():\n",
    "            if word in word_freq:\n",
    "                tf = word_freq[word] / total_words\n",
    "                idf = idf_dict.get(word, 0.0)\n",
    "                vector[index] = tf * idf\n",
    "        \n",
    "        return vector\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка в TF-IDF: {e}\")\n",
    "        return [0.0] * len(vocab) if vocab else []\n",
    "\n",
    "    \n",
    "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
    "    try:\n",
    "        text = \"the quick brown\"\n",
    "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
    "\n",
    "        if not isinstance(result, list):\n",
    "            return False\n",
    "\n",
    "        expected_length = len(vocab)\n",
    "        if len(result) != expected_length:\n",
    "            return False\n",
    "\n",
    "        for val in result:\n",
    "            if not isinstance(val, float):\n",
    "                return False\n",
    "\n",
    "        print(\"TF-IDF test PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"TF-IDF test FAILED: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "GKIyS724T0XH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF test PASSED\n"
     ]
    }
   ],
   "source": [
    "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0f9FZCrX5_s"
   },
   "source": [
    "## Задание 4 (1 балл)\n",
    "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
    "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
    "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
    "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
    "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "SUg6K2-wTwr6"
   },
   "outputs": [],
   "source": [
    "def ppmi_vectorization(\n",
    "    text: str,\n",
    "    corpus: List[str] = None,\n",
    "    vocab: List[str] = None,\n",
    "    vocab_index: Dict[str, int] = None,\n",
    "    window_size: int = 2\n",
    ") -> List[float]:\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    \n",
    "    N_matrix = np.zeros((vocab_size, vocab_size), dtype=int) # Матрица совместной встречаемости\n",
    "    word_counts = defaultdict(int)  # Счетчик частот слов\n",
    "    total_pairs = 0  # Общее количество пар\n",
    "    \n",
    "    for doc in corpus: # Проходим по каждому документу в корпусе\n",
    "        words = normalize_pretokenize_text(doc)\n",
    "        \n",
    "        for i, target_word in enumerate(words):\n",
    "                \n",
    "            target_idx = vocab_index[target_word]\n",
    "            word_counts[target_word] += 1\n",
    "            \n",
    "            # Задаем начало и конец контекстного окна\n",
    "            start_context_window = max(0, i - window_size)\n",
    "            end_context_window = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start_context_window, end_context_window):\n",
    "                if j == i: # Пропускаем само слово\n",
    "                    continue\n",
    "                    \n",
    "                context_word = words[j]\n",
    "                if context_word in vocab_index:\n",
    "                    context_idx = vocab_index[context_word]\n",
    "                    N_matrix[target_idx][context_idx] += 1 # Увеличиваем счетчик в матрице совместимости\n",
    "                    total_pairs += 1\n",
    "    \n",
    "    # Теперь обрабатываем входной текст\n",
    "    text_words = normalize_pretokenize_text(text)\n",
    "    vector = [0.0] * vocab_size\n",
    "    \n",
    "    results = [] # Здесь будем хранить PPMI векторы каждого слова\n",
    "    \n",
    "    for target_word in text_words: # Для каждого слова во входном тексте\n",
    "        \n",
    "        target_idx = vocab_index[target_word]\n",
    "        word_vector = [0.0] * vocab_size\n",
    "        \n",
    "        for context_word, context_idx in vocab_index.items(): # Вычисляем PPMI для каждого слова словаря\n",
    "            # Сколько раз target_word и context_word встречались вместе\n",
    "            ij_together = N_matrix[target_idx][context_idx]\n",
    "            \n",
    "            if ij_together > 0:\n",
    "                n_i = word_counts.get(target_word, 0)\n",
    "                n_j = word_counts.get(context_word, 0)\n",
    "                \n",
    "                if n_i > 0 and n_j > 0:\n",
    "                    pmi = math.log((ij_together * total_pairs) / (n_i * n_j))\n",
    "                    ppmi = max(0.0, pmi)\n",
    "                    word_vector[context_idx] = ppmi\n",
    "        \n",
    "        results.append(word_vector)\n",
    "    \n",
    "   \n",
    "    if results:  \n",
    "        for i in range(vocab_size):\n",
    "            total = sum(word_vector[i] for word_vector in results)\n",
    "            vector[i] = total / len(results)\n",
    "    \n",
    "    return vector \n",
    "\n",
    "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
    "    try:\n",
    "        text = \"quick brown fox\"\n",
    "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
    "\n",
    "        if not isinstance(result, list):\n",
    "            return False\n",
    "\n",
    "        expected_length = len(vocab)\n",
    "        if len(result) != expected_length:\n",
    "            return False\n",
    "\n",
    "        for val in result:\n",
    "            if not isinstance(val, float):\n",
    "                return False\n",
    "\n",
    "        print(\"PPMI test PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"PPMI test FAILED: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "HgHmNZy75XFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI test PASSED\n"
     ]
    }
   ],
   "source": [
    "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FK29va3PBH_8"
   },
   "source": [
    "## Задание 5 (1 балл)\n",
    "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "tOe8dRLl5eqN"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gensim.downloader as api # Использую gensim вместо fasttext\n",
    "\n",
    "def get_fasttext_embeddings(text: str, model_path: str = None, model: any = None) -> List[np.ndarray]:\n",
    "\n",
    "    if model is None:\n",
    "        model = api.load('fasttext-wiki-news-subwords-300')\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    word_vectors = []  # Здесь хранми векторы каждого слова\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model.wv[word]\n",
    "            word_vectors.append(vector)\n",
    "            \n",
    "        except KeyError: # Обрабатываем отсутствие слова в модели (ну а вдруг)\n",
    "            \n",
    "            vector_size = model.wv.vector_size  # Узнаем размерность векторов модели\n",
    "            zero_vector = np.zeros(vector_size)  # Создаем вектор из нулей\n",
    "            \n",
    "            word_vectors.append(zero_vector)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "A9GXy6n0AtsZ"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(\n",
    "    text: str,\n",
    "    model_name: str = 'bert-base-uncased',\n",
    "    pool_method: str = 'cls'\n",
    ") -> np.ndarray:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',           # Возвращать PyTorch тензоры\n",
    "        truncation=True,               # Обрезать длинные тексты\n",
    "        max_length=512,                # Максимальная длина BERT\n",
    "        padding=True,                  # Дополнять до одинаковой длины\n",
    "        add_special_tokens=True        # Добавлять [CLS] и [SEP]\n",
    "    )\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooler_output = outputs.pooler_output\n",
    "   \n",
    "    if pool_method == 'cls':\n",
    "        cls_embedding = pooler_output[0].numpy()\n",
    "        result_vector = cls_embedding\n",
    "        \n",
    "    elif pool_method == 'mean':\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        token_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "        sum_embeddings = torch.sum(token_embeddings, dim=1)\n",
    "        sum_mask = torch.sum(attention_mask, dim=1, keepdim=True)\n",
    "        \n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        result_vector = mean_embeddings[0].numpy()  \n",
    "        \n",
    "    elif pool_method == 'max':\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_embeddings = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "        token_embeddings[token_embeddings == 0] = -float('inf')\n",
    "        \n",
    "        max_embeddings = torch.max(token_embeddings, dim=1)[0]\n",
    "        result_vector = max_embeddings[0].numpy()\n",
    "\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_KoKolrD49R"
   },
   "source": [
    "## Задание 6 (1.5 балла)\n",
    "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "zsc98L8JE8G-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def vectorize_with_vocab(\n",
    "    texts: List[str],\n",
    "    labels: List[int],\n",
    "    vocab: List[str],\n",
    "    vocab_index: Dict[str, int],\n",
    "    vectorizer_type: str = \"bow\"\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Векторизует тексты используя заданный словарь\n",
    "    \"\"\"\n",
    "    vectorized_data = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        try:\n",
    "            if vectorizer_type == \"one_hot\":\n",
    "                vector = one_hot_vectorization(text, vocab, vocab_index)\n",
    "                \n",
    "            elif vectorizer_type == \"bow\":\n",
    "                bow_dict = bag_of_words_vectorization(text)\n",
    "                vector = [bow_dict.get(word, 0) for word in vocab]  # Используем общий vocab\n",
    "                \n",
    "            elif vectorizer_type == \"tfidf\":\n",
    "                vector = tf_idf_vectorization(text, texts, vocab, vocab_index)\n",
    "                \n",
    "            elif vectorizer_type == \"ppmi\":\n",
    "                vector = ppmi_vectorization(text, texts, vocab, vocab_index)\n",
    "                \n",
    "            elif vectorizer_type == \"fasttext\":\n",
    "                embeddings = get_fasttext_embeddings(text)\n",
    "                if embeddings and len(embeddings) > 0:\n",
    "                    avg_embedding = np.mean(embeddings, axis=0)\n",
    "                    vector = avg_embedding.tolist()\n",
    "                else:\n",
    "                    vector = [0.0] * 300\n",
    "                    \n",
    "            elif vectorizer_type == \"bert\":\n",
    "                embedding = get_bert_embeddings(text)\n",
    "                vector = embedding.tolist()\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
    "            \n",
    "            vectorized_data.append(vector)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Ошибка при векторизации текста {i}: {e}\")\n",
    "            vector_size = len(vocab) if vectorizer_type in [\"one_hot\", \"bow\", \"tfidf\", \"ppmi\"] else 300\n",
    "            zero_vector = [0.0] * vector_size\n",
    "            vectorized_data.append(zero_vector)\n",
    "    \n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    dataset_name: str = \"imdb\",\n",
    "    split: str = \"train\", \n",
    "    sample_size: int = 2500\n",
    ") -> Tuple[List[str], List[int]]:\n",
    "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
    "    \n",
    "    if sample_size:\n",
    "        # Разделяем датасет по классам\n",
    "        negative_examples = [item for item in dataset if item['label'] == 0]\n",
    "        positive_examples = [item for item in dataset if item['label'] == 1]\n",
    "        \n",
    "        # Берем равное количество из каждого класса\n",
    "        half_size = sample_size // 2\n",
    "        selected_negative = negative_examples[:half_size]\n",
    "        selected_positive = positive_examples[:half_size]\n",
    "        \n",
    "        # Объединяем и перемешиваем\n",
    "        balanced_dataset = selected_negative + selected_positive\n",
    "        import random\n",
    "        random.shuffle(balanced_dataset)\n",
    "        \n",
    "        texts = [item['text'] for item in balanced_dataset]\n",
    "        labels = [item['label'] for item in balanced_dataset]\n",
    "    else:\n",
    "        # Берем весь датасет\n",
    "        texts = [item['text'] for item in dataset]\n",
    "        labels = [item['label'] for item in dataset]\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def vectorize_dataset(\n",
    "    dataset_name: str = \"imdb\",\n",
    "    vectorizer_type: str = \"bow\",\n",
    "    split: str = \"train\",\n",
    "    sample_size: int = 2500,\n",
    "    vocab: List[str] = None,\n",
    "    vocab_index: Dict[str, int] = None\n",
    ") -> Tuple[Any, List, List]:\n",
    "   \n",
    "    texts, labels = load_dataset(dataset_name, split, sample_size)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab, vocab_index = build_vocab(texts)\n",
    "    \n",
    "    vectorized_data = vectorize_with_vocab(texts, labels, vocab, vocab_index, vectorizer_type)\n",
    "    \n",
    "    return vocab, vocab_index, vectorized_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "DRRw01XiBg6H"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    embeddings_method=\"bow\",\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    cv_folds=1,\n",
    "    sample_size=2500\n",
    "):\n",
    "    print(f\"Обучение с методом {embeddings_method.upper()}\")\n",
    "  \n",
    "    vocab, vocab_index, X_train_full, y_train_full = vectorize_dataset(\n",
    "        \"imdb\", embeddings_method, \"train\", sample_size\n",
    "    )\n",
    "    \n",
    "  \n",
    "    _, _, X_test, y_test = vectorize_dataset(\n",
    "        \"imdb\", embeddings_method, \"test\", sample_size, vocab, vocab_index\n",
    "    )\n",
    "    \n",
    "    X_train_full = np.array(X_train_full)\n",
    "    y_train_full = np.array(y_train_full)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, \n",
    "        test_size=val_size, \n",
    "        random_state=42,\n",
    "        stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=4,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train, y_train, \n",
    "            cv=kf, \n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        cv_scores = np.array([0.0])\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "    )\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nРезультаты на Validation Set:\")\n",
    "    print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    print(f\"Результаты на Test Set:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'method': embeddings_method,\n",
    "        'cv_mean_accuracy': cv_scores.mean() if len(cv_scores) > 0 else 0.0,\n",
    "        'cv_std': cv_scores.std() if len(cv_scores) > 0 else 0.0,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1': val_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1': test_f1,\n",
    "        'feature_dim': X_train.shape[1],\n",
    "        'train_size': X_train.shape[0],\n",
    "        'vocab_size': len(vocab)\n",
    "    }\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "naMqAkjqFHAe"
   },
   "outputs": [],
   "source": [
    "def compare_succesfull_methods(): # К сожанелию не получилось добиться быстрых результатов для последних трех методов\n",
    "    \n",
    "    methods = [\"bow\", \"one_hot\", \"tfidf\"]  #[\"ppmi\", \"fasttext\", \"bert\"]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for method in methods:\n",
    "        try:\n",
    "            \n",
    "            model, results = train(embeddings_method=method)\n",
    "            all_results.append(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    for result in all_results:\n",
    "        print(f\"{result['method']:<12} {result['test_accuracy']:<14.4f} \"\n",
    "              f\"{result['test_f1']:<12.4f} {result['cv_mean_accuracy']:<12.4f} \"\n",
    "              f\"{result['feature_dim']:<10}\")\n",
    "    best_result = max(all_results, key=lambda x: x['test_accuracy'])\n",
    "    print(f\"\\n лучший метод {best_result['method']}\")\n",
    "    print(f\"Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "    print(f\"Test F1-Score: {best_result['test_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение с методом BOW\n",
      "0:\tlearn: 0.6836860\ttest: 0.6853198\tbest: 0.6853198 (0)\ttotal: 54ms\tremaining: 27s\n",
      "100:\tlearn: 0.4615247\ttest: 0.5137950\tbest: 0.5137950 (100)\ttotal: 4.76s\tremaining: 18.8s\n",
      "200:\tlearn: 0.3631448\ttest: 0.4518181\tbest: 0.4518181 (200)\ttotal: 9.56s\tremaining: 14.2s\n",
      "300:\tlearn: 0.2889382\ttest: 0.4160813\tbest: 0.4160813 (300)\ttotal: 14.3s\tremaining: 9.48s\n",
      "400:\tlearn: 0.2392126\ttest: 0.3995551\tbest: 0.3995394 (396)\ttotal: 19.1s\tremaining: 4.7s\n",
      "499:\tlearn: 0.2043874\ttest: 0.3889598\tbest: 0.3889598 (499)\ttotal: 23.5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.388959803\n",
      "bestIteration = 499\n",
      "\n",
      "\n",
      "Результаты на Validation Set:\n",
      "Accuracy: 0.8540\n",
      "F1-Score: 0.8537\n",
      "Результаты на Test Set:\n",
      "Accuracy: 0.8224\n",
      "F1-Score: 0.8224\n",
      "Обучение с методом ONE_HOT\n",
      "0:\tlearn: 0.6856473\ttest: 0.6879025\tbest: 0.6879025 (0)\ttotal: 65ms\tremaining: 32.4s\n",
      "100:\tlearn: 0.4672740\ttest: 0.5044890\tbest: 0.5044890 (100)\ttotal: 6.71s\tremaining: 26.5s\n",
      "200:\tlearn: 0.3683094\ttest: 0.4449037\tbest: 0.4449037 (200)\ttotal: 13.4s\tremaining: 19.9s\n",
      "300:\tlearn: 0.2960765\ttest: 0.4066910\tbest: 0.4066910 (300)\ttotal: 19.6s\tremaining: 12.9s\n",
      "400:\tlearn: 0.2473186\ttest: 0.3874309\tbest: 0.3874309 (400)\ttotal: 25.5s\tremaining: 6.3s\n",
      "499:\tlearn: 0.2127737\ttest: 0.3744234\tbest: 0.3743453 (497)\ttotal: 31.2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3743452914\n",
      "bestIteration = 497\n",
      "\n",
      "Shrink model to first 498 iterations.\n",
      "\n",
      "Результаты на Validation Set:\n",
      "Accuracy: 0.8400\n",
      "F1-Score: 0.8398\n",
      "Результаты на Test Set:\n",
      "Accuracy: 0.8252\n",
      "F1-Score: 0.8252\n",
      "Обучение с методом TFIDF\n",
      "0:\tlearn: 0.6823878\ttest: 0.6852307\tbest: 0.6852307 (0)\ttotal: 87ms\tremaining: 43.4s\n",
      "100:\tlearn: 0.4405246\ttest: 0.5136139\tbest: 0.5136139 (100)\ttotal: 8.41s\tremaining: 33.2s\n",
      "200:\tlearn: 0.3347072\ttest: 0.4643192\tbest: 0.4643192 (200)\ttotal: 16.5s\tremaining: 24.5s\n",
      "300:\tlearn: 0.2538029\ttest: 0.4355587\tbest: 0.4355374 (296)\ttotal: 24.6s\tremaining: 16.3s\n",
      "400:\tlearn: 0.1996487\ttest: 0.4181960\tbest: 0.4181960 (400)\ttotal: 32.6s\tremaining: 8.05s\n",
      "499:\tlearn: 0.1615507\ttest: 0.4105633\tbest: 0.4105633 (499)\ttotal: 40.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.410563329\n",
      "bestIteration = 499\n",
      "\n",
      "\n",
      "Результаты на Validation Set:\n",
      "Accuracy: 0.8060\n",
      "F1-Score: 0.8057\n",
      "Результаты на Test Set:\n",
      "Accuracy: 0.8140\n",
      "F1-Score: 0.8139\n",
      "bow          0.8224         0.8224       0.0000       25844     \n",
      "one_hot      0.8252         0.8252       0.0000       25844     \n",
      "tfidf        0.8140         0.8139       0.0000       25844     \n",
      "\n",
      " лучший метод one_hot\n",
      "Test Accuracy: 0.8252\n",
      "Test F1-Score: 0.8252\n"
     ]
    }
   ],
   "source": [
    "compare_succesfull_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
